

import { useEffect, useRef, useState } from "react";
import type { Socket } from "socket.io-client";

declare global {
  interface Window {
    SherpaModule: any;      
    OnlineRecognizer: any;  
    AudioContext: any;
    webkitAudioContext: any;
    Module: any;
  }
}

export function useZipformer(
  socket: Socket | null,
  roomId: string | null,
  displayName: string,
  enabled: boolean
) {
  const [isModelReady, setIsModelReady] = useState(false);
  const [statusText, setStatusText] = useState("‚è≥ ƒêang ƒë·ª£i..."); 
  
  const recognizerRef = useRef<any>(null);
  const audioCtxRef = useRef<AudioContext | null>(null);
  const workletRef = useRef<AudioWorkletNode | null>(null);
  const streamRef = useRef<any>(null);
  const lastTextRef = useRef<string>("");
  const isInitializingRef = useRef(false);

  useEffect(() => {
    if (!enabled || !roomId || !socket) return;
    if (isInitializingRef.current) return;

    isInitializingRef.current = true;
    let aborted = false;

    // --- H√ÄM D·ªåN D·∫∏P ---
    const cleanup = () => {
      if (workletRef.current) {
        workletRef.current.port.onmessage = null;
        workletRef.current.disconnect();
        workletRef.current = null;
      }
      if (streamRef.current) {
        try { streamRef.current.free(); } catch {}
        streamRef.current = null;
      }
      if (audioCtxRef.current && audioCtxRef.current.state !== "closed") {
        audioCtxRef.current.close();
        audioCtxRef.current = null;
      }
      isInitializingRef.current = false;
      if (!aborted) setIsModelReady(false);
    };

    // --- LOAD SCRIPT ---
    const loadScriptAndInitWasm = (): Promise<void> => {
        return new Promise((resolve, reject) => {
            if (window.SherpaModule && window.OnlineRecognizer) {
                resolve();
                return;
            }
            window.Module = {
                onRuntimeInitialized: () => {
                    window.SherpaModule = window.Module;
                    if (typeof window.OnlineRecognizer === 'undefined') {
                        // @ts-ignore
                        window.OnlineRecognizer = OnlineRecognizer; 
                    }
                    resolve();
                },
                locateFile: (path: string) => path.endsWith('.wasm') ? '/lib/sherpa-onnx-wasm-main-asr.wasm' : path
            };
            const script = document.createElement('script');
            script.src = "/lib/sherpa-onnx.js";
            script.async = true;
            script.onerror = () => reject(new Error("L·ªói t·∫£i script sherpa-onnx.js"));
            document.body.appendChild(script);
        });
    };

    const initSherpa = async () => {
      try {
        setStatusText("‚è≥ ƒêang t·∫£i th∆∞ vi·ªán...");
        await loadScriptAndInitWasm();
        if (aborted) return;
        
        const SherpaModule = window.SherpaModule;
        const OnlineRecognizer = window.OnlineRecognizer;

        // --- MOUNT FILE ---
        const mountFile = async (url: string, filename: string) => {
           try {
             if (SherpaModule.FS.analyzePath("/" + filename).exists) return;
           } catch(e) {}
           const res = await fetch(url);
           if (!res.ok) throw new Error(`L·ªói t·∫£i ${url}`);
           const buffer = await res.arrayBuffer();
           SherpaModule.FS_createDataFile("/", filename, new Uint8Array(buffer), true, false, true);
        };

        setStatusText("üì¶ ƒêang t·∫£i Models...");
        await Promise.all([
          mountFile("/models/zipformer-en/encoder.onnx", "encoder.onnx"),
          mountFile("/models/zipformer-en/decoder.onnx", "decoder.onnx"),
          mountFile("/models/zipformer-en/joiner.onnx", "joiner.onnx"),
          mountFile("/models/zipformer-en/tokens.txt", "tokens.txt"),
        ]);

        setStatusText("‚öôÔ∏è ƒêang c·∫•u h√¨nh...");

        // --- C·∫§U H√åNH NG·∫ÆT C√ÇU (QUAN TR·ªåNG) ---
        const config = {
          featConfig: {
            sampleRate: 16000,
            featureDim: 80,
          },
          modelConfig: {
            transducer: {
               encoder: '/encoder.onnx', 
               decoder: '/decoder.onnx',
               joiner: '/joiner.onnx'
            },
            tokens: "/tokens.txt",
            numThreads: 1,
            provider: "cpu",
            debug: 0, 
            modelType: "zipformer",
          },
          decodingMethod: "greedy_search",
          enableEndpoint: 1, // B·∫≠t t√≠nh nƒÉng ph√°t hi·ªán ƒëi·ªÉm cu·ªëi
          
          // Rule 1: N·∫øu im l·∫∑ng trong 2.0 gi√¢y sau khi ƒë√£ n√≥i g√¨ ƒë√≥ -> Ng·∫Øt c√¢u
          rule1MinTrailingSilence: 2.0, 
          
          // Rule 2: N·∫øu im l·∫∑ng trong 1.2 gi√¢y (d·ª± ph√≤ng cho c√¢u ng·∫Øn) -> Ng·∫Øt
          rule2MinTrailingSilence: 1.2, 
          
          // Rule 3: N·∫øu c√¢u d√†i qu√° 20s m√† ch∆∞a ng·∫Øt -> B·∫Øt bu·ªôc ng·∫Øt
          rule3MinUtteranceLength: 20,
        };

        if (!recognizerRef.current) {
             recognizerRef.current = new OnlineRecognizer(config, SherpaModule);
        }
        
        // --- AUDIO SETUP ---
        const AudioContext = window.AudioContext || window.webkitAudioContext;
        const audioCtx = new AudioContext({ sampleRate: 16000 });
        audioCtxRef.current = audioCtx;
        
        try {
            await audioCtx.audioWorklet.addModule("/lib/audio-processor.js");
        } catch(e) {
            await audioCtx.audioWorklet.addModule("/audio-processor.js");
        }

        const userMedia = await navigator.mediaDevices.getUserMedia({ audio: true });
        const source = audioCtx.createMediaStreamSource(userMedia);
        const worklet = new AudioWorkletNode(audioCtx, "audio-processor");
        workletRef.current = worklet;
        
        source.connect(worklet);
        worklet.connect(audioCtx.destination);

        // T·∫°o Stream m·ªõi
        if (streamRef.current) {
            try { streamRef.current.free(); } catch {}
        }
        streamRef.current = recognizerRef.current.createStream();
        
        setIsModelReady(true);
        setStatusText("üü¢ ƒêang nghe...");

        // --- X·ª¨ L√ù K·∫æT QU·∫¢ ---
        worklet.port.onmessage = (event) => {
           if (aborted) return;
           const float32Audio = event.data;
           
           if (recognizerRef.current && streamRef.current) {
              streamRef.current.acceptWaveform(16000, float32Audio);
              
              // Decode li√™n t·ª•c
              while (recognizerRef.current.isReady(streamRef.current)) {
                 recognizerRef.current.decode(streamRef.current);
              }

              // L·∫•y text hi·ªán t·∫°i
              const result = recognizerRef.current.getResult(streamRef.current);
              const text = result.text;

              if (text && text.length > 0) {
                 const textToSend = text.trim();
                 
                 // G·ª≠i text realtime (ƒë·ªÉ hi·ªÉn th·ªã ch·ªØ ƒëang ch·∫°y)
                 if (textToSend !== lastTextRef.current) {
                    if (socket && socket.connected) {
                       socket.emit("send-subtitle", { 
                           roomId, 
                           text: textToSend, 
                           displayName,
                           isFinal: false // ƒê√°nh d·∫•u l√† ch∆∞a ch·ªët c√¢u
                       });
                    }
                    lastTextRef.current = textToSend;
                 }
              }

              // --- LOGIC NG·∫ÆT C√ÇU (QUAN TR·ªåNG) ---
              if (recognizerRef.current.isEndpoint(streamRef.current)) {
                  
                  // 1. L·∫•y to√†n b·ªô c√¢u ch·ªët h·∫°
                  const finalText = recognizerRef.current.getResult(streamRef.current).text;
                  
                  if (finalText && finalText.length > 0) {
                    
                      if (socket && socket.connected) {
                          socket.emit("send-subtitle", { 
                              roomId, 
                              text: finalText, 
                              displayName,
                              isFinal: true // ƒê√°nh d·∫•u ƒë√¢y l√† c√¢u ho√†n ch·ªânh
                          });
                      }
                  }

                  recognizerRef.current.reset(streamRef.current);
                  lastTextRef.current = ""; // Reset bi·∫øn t·∫°m
              }
           }
        };

      } catch (err: any) {
        console.error("‚ùå [Zipformer Error]:", err);
        setStatusText("L·ªói: " + err.message);
        cleanup();
      }
    };

    initSherpa();

    return () => {
      aborted = true;
      cleanup();
    };
  }, [enabled, roomId, socket, displayName]);

  return { isModelReady, statusText };
}