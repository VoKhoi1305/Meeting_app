

import { useEffect, useRef, useState } from "react";
import type { Socket } from "socket.io-client";

declare global {
  interface Window {
    sherpaOnnx: any;
  }
}

export function useZipformerVI(
  socket: Socket | null,
  roomId: string | null,
  displayName: string,
  enabled: boolean
) {
  const [isModelReady, setIsModelReady] = useState(false);
  const recognizerRef = useRef<any>(null);
  const audioCtxRef = useRef<AudioContext | null>(null);
  const workletRef = useRef<AudioWorkletNode | null>(null);
  const streamRef = useRef<any>(null);
  const isInitializingRef = useRef<boolean>(false);

  useEffect(() => {
    let aborted = false;
    let checkInterval: ReturnType<typeof setInterval>;

    const internalCleanup = () => {
      if (workletRef.current) {
        workletRef.current.port.onmessage = null; // Ngáº¯t listener
        workletRef.current.disconnect();
        workletRef.current = null;
      }
      if (streamRef.current) {
        try { streamRef.current.free(); } catch (e) {}
        streamRef.current = null;
      }
      if (recognizerRef.current) {
        try { recognizerRef.current.free(); } catch (e) {}
        recognizerRef.current = null;
      }
      if (audioCtxRef.current) {
        if (audioCtxRef.current.state !== "closed") {
          audioCtxRef.current.close();
        }
        audioCtxRef.current = null;
      }
      isInitializingRef.current = false;
      if (!aborted) setIsModelReady(false);
    };

    const start = async () => {
      if (isInitializingRef.current || aborted) return;
      if (audioCtxRef.current?.state === "running") return;

      try {
        isInitializingRef.current = true;
        const sherpa = window.sherpaOnnx;

        // 1. Setup AudioContext
        const audioCtx = new AudioContext({ sampleRate: 16000 });
        audioCtxRef.current = audioCtx;
        
        // Äáº£m báº£o file nÃ y náº±m Ä‘Ãºng á»Ÿ public/audio-processor.js
        await audioCtx.audioWorklet.addModule("/audio-processor.js");
        if (aborted) { internalCleanup(); return; }

        // 2. Setup Mic
        const mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        if (aborted) {
            mediaStream.getTracks().forEach(t => t.stop());
            internalCleanup();
            return;
        }

        const source = audioCtx.createMediaStreamSource(mediaStream);
        const worklet = new AudioWorkletNode(audioCtx, "audio-processor");
        workletRef.current = worklet;
        source.connect(worklet);
        worklet.connect(audioCtx.destination);

        // 3. Setup Model Zipformer (Tiáº¿ng Viá»‡t)
        const recognizer = sherpa.createOnlineRecognizer({
          modelConfig: {
            zipformer: {
              encoder: "/models/zipformer-vi/encoder-epoch-12-avg-8.onnx",
              decoder: "/models/zipformer-vi/decoder-epoch-12-avg-8.onnx",
              joiner: "/models/zipformer-vi/joiner-epoch-12-avg-8.onnx",
            },
            tokens: "/models/zipformer-vi/tokens.txt",
            provider: "wasm",
            numThreads: 2,
          },
          decodingMethod: "greedy_search",
          enableEndpoint: true,
        });

        recognizerRef.current = recognizer;
        streamRef.current = recognizer.createStream();

        // 4. Xá»­ lÃ½ nháº­n dáº¡ng
        worklet.port.onmessage = (e) => {
          if (aborted) return;
          const samples = e.data as Float32Array;

          if (recognizerRef.current && streamRef.current) {
            streamRef.current.acceptWaveform(16000, samples);
            
            while (recognizerRef.current.isReady(streamRef.current)) {
              recognizerRef.current.decode(streamRef.current);
            }

            const result = recognizerRef.current.getResult(streamRef.current);
            const text = result.text;

            // ðŸ‘‡ QUAN TRá»ŒNG: Sá»­a logic gá»­i Socket Ä‘á»ƒ khá»›p vá»›i useSpeechToText
            if (text && text.length > 0 && socket?.connected) {
                // Kiá»ƒm tra xem text cÃ³ ná»™i dung thá»±c khÃ´ng (Ä‘Ã´i khi model tráº£ vá» chuá»—i rá»—ng)
                const textToSend = text.trim();
                
                if (textToSend) {
                    socket.emit("send-subtitle", { // Äá»•i tá»« 'subtitle' -> 'send-subtitle'
                        roomId, 
                        text: textToSend,
                        displayName: displayName // Äá»•i tá»« 'speaker' -> 'displayName'
                    });
                    
                }
            }
          }
        };

        if (!aborted) {
            setIsModelReady(true);
            console.log("âœ… Zipformer AI Ä‘Ã£ sáºµn sÃ ng vÃ  Ä‘ang nghe!");
        }
        isInitializingRef.current = false;

      } catch (err) {
        console.error("âŒ Lá»—i khá»Ÿi táº¡o Zipformer:", err);
        internalCleanup();
      }
    };

    // Kiá»ƒm tra thÆ° viá»‡n Sherpa Ä‘Ã£ load chÆ°a
    if (enabled && socket && roomId) {
        const checkLibrary = () => {
            if (aborted) return;
            if (typeof window.sherpaOnnx !== "undefined") {
                clearInterval(checkInterval);
                start();
            }
        };

        if (typeof window.sherpaOnnx !== "undefined") {
            start();
        } else {
            console.warn("â³ Äang Ä‘á»£i thÆ° viá»‡n Sherpa load...");
            checkInterval = setInterval(checkLibrary, 500);
        }
    } else {
        internalCleanup();
    }

    return () => {
      aborted = true;
      if (checkInterval) clearInterval(checkInterval);
      internalCleanup();
    };
  }, [enabled, roomId, socket, displayName]); // ThÃªm displayName vÃ o deps Ä‘á»ƒ cáº­p nháº­t tÃªn khi Ä‘á»•i

  return { isModelReady };
}