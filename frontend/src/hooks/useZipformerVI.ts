// import { useEffect, useRef } from "react";
// import type { Socket } from "socket.io-client";

// // Khai b√°o type cho window
// declare global {
//   interface Window {
//     sherpaOnnx: any;
//   }
// }

// export function useZipformerVI(
//   socket: Socket | null,
//   roomId: string | null,
//   displayName: string,
//   enabled: boolean
// ) {
//   const recognizerRef = useRef<any>(null);
//   const audioCtxRef = useRef<AudioContext | null>(null);
//   const workletRef = useRef<AudioWorkletNode | null>(null);
//   const streamRef = useRef<any>(null);
  
//   // Ref n√†y ƒë·ªÉ ch·∫∑n vi·ªác start ch·∫°y ch·ªìng ch√©o
//   const isInitializingRef = useRef<boolean>(false);

//   useEffect(() => {
//     // Bi·∫øn c·ªù ƒë·ªÉ ki·ªÉm so√°t vi·ªác h·ªßy khi component unmount
//     let aborted = false;
//     let checkInterval: ReturnType<typeof setInterval>;

//     // H√†m d·ªçn d·∫πp n·ªôi b·ªô
//     const internalCleanup = () => {
//       // 1. Ng·∫Øt k·∫øt n·ªëi Worklet
//       if (workletRef.current) {
//         workletRef.current.disconnect();
//         workletRef.current = null;
//       }

//       // 2. Gi·∫£i ph√≥ng Stream Sherpa
//       if (streamRef.current) {
//         try {
//             streamRef.current.free();
//         } catch(e) { /* B·ªè qua l·ªói n·∫øu ƒë√£ free */ }
//         streamRef.current = null;
//       }

//       // 3. Gi·∫£i ph√≥ng Recognizer
//       if (recognizerRef.current) {
//         try {
//             recognizerRef.current.free();
//         } catch(e) { /* B·ªè qua l·ªói */ }
//         recognizerRef.current = null;
//       }

//       // 4. ƒê√≥ng AudioContext
//       if (audioCtxRef.current) {
//         if (audioCtxRef.current.state !== "closed") {
//           audioCtxRef.current.close();
//         }
//         audioCtxRef.current = null;
//       }
      
//       isInitializingRef.current = false;
//     };

//     const start = async () => {
//       // N·∫øu ƒëang kh·ªüi t·∫°o ho·∫∑c ƒë√£ b·ªã h·ªßy th√¨ d·ª´ng ngay
//       if (isInitializingRef.current || aborted) return;
      
//       // N·∫øu AudioContext ƒëang ch·∫°y t·ªët th√¨ kh√¥ng t·∫°o m·ªõi
//       if (audioCtxRef.current?.state === "running") return;

//       try {
//         isInitializingRef.current = true;
//         const sherpa = window.sherpaOnnx;

//         // --- B∆Ø·ªöC 1: Kh·ªüi t·∫°o AudioContext ---
//         const audioCtx = new AudioContext({ sampleRate: 16000 });
//         audioCtxRef.current = audioCtx;

//         // T·∫£i Worklet (ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ª´ public)
//         await audioCtx.audioWorklet.addModule("/audio-processor.js");

//         // KI·ªÇM TRA L·∫†I: N·∫øu trong l√∫c await m√† component b·ªã h·ªßy -> D·ª´ng ngay
//         if (aborted) {
//             internalCleanup();
//             return;
//         }

//         // --- B∆Ø·ªöC 2: Xin quy·ªÅn Micro ---
//         const mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
//         if (aborted) {
//              mediaStream.getTracks().forEach(track => track.stop());
//              internalCleanup();
//              return;
//         }

//         const source = audioCtx.createMediaStreamSource(mediaStream);
//         const worklet = new AudioWorkletNode(audioCtx, "audio-processor");
//         workletRef.current = worklet;

//         source.connect(worklet);
//         worklet.connect(audioCtx.destination);

//         // --- B∆Ø·ªöC 3: Kh·ªüi t·∫°o Sherpa AI ---
//         // C·∫•u h√¨nh Model Ti·∫øng Vi·ªát
//         const recognizer = sherpa.createOnlineRecognizer({
//           modelConfig: {
//             zipformer: {
//               encoder: "/models/zipformer-vi/encoder-epoch-12-avg-8.onnx",
//               decoder: "/models/zipformer-vi/decoder-epoch-12-avg-8.onnx",
//               joiner: "/models/zipformer-vi/joiner-epoch-12-avg-8.onnx",
//             },
//             tokens: "/models/zipformer-vi/tokens.txt",
//             provider: "wasm",
//             numThreads: 2,
//           },
//           decodingMethod: "greedy_search",
//           enableEndpoint: true,
//         });

//         recognizerRef.current = recognizer;
//         const stream = recognizer.createStream();
//         streamRef.current = stream;

//         // --- B∆Ø·ªöC 4: X·ª≠ l√Ω s·ª± ki·ªán Audio ---
//         worklet.port.onmessage = (e) => {
//           if (aborted) return; // N·∫øu ƒë√£ h·ªßy th√¨ kh√¥ng x·ª≠ l√Ω tin nh·∫Øn c≈©
          
//           const samples = e.data as Float32Array;
          
//           // Ki·ªÉm tra k·ªπ c√°c object c√≤n t·ªìn t·∫°i kh√¥ng tr∆∞·ªõc khi d√πng
//           if (recognizerRef.current && streamRef.current) {
//             streamRef.current.acceptWaveform(16000, samples);
            
//             while (recognizerRef.current.isReady(streamRef.current)) {
//               recognizerRef.current.decode(streamRef.current);
//             }
            
//             const result = recognizerRef.current.getResult(streamRef.current);
            
//             // Ch·ªâ g·ª≠i khi c√≥ text v√† socket ƒëang k·∫øt n·ªëi
//             if (result.text && result.text.length > 0 && socket?.connected) {
//               const text = result.text.trim();
//               if (text) {
//                   // G·ª≠i k·∫øt qu·∫£ l√™n Server
//                   socket.emit("subtitle", { 
//                       roomId, 
//                       speaker: displayName, 
//                       text: text 
//                   });
//                   // Quan tr·ªçng: Reset stream text sau khi g·ª≠i ƒë·ªÉ tr√°nh l·∫∑p l·∫°i c√¢u c≈©
//                   // (T√πy thu·ªôc v√†o logic hi·ªÉn th·ªã c·ªßa b·∫°n, nh∆∞ng v·ªõi stream li√™n t·ª•c th√¨ Sherpa t·ª± qu·∫£n l√Ω)
//               }
//             }
//           }
//         };

//         isInitializingRef.current = false;
//         console.log("‚úÖ Sherpa-ONNX (Ti·∫øng Vi·ªát) ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng!");

//       } catch (err) {
//         console.error("‚ùå L·ªói kh·ªüi t·∫°o Zipformer:", err);
//         internalCleanup();
//       }
//     };

//     // Logic ki·ªÉm tra th∆∞ vi·ªán v√† b·∫Øt ƒë·∫ßu
//     if (enabled && socket && roomId) {
//         const checkLibrary = () => {
//             if (aborted) return;
            
//             if (typeof window.sherpaOnnx !== "undefined") {
//                 if (checkInterval) clearInterval(checkInterval);
//                 start();
//             }
//         };

//         // Ki·ªÉm tra ngay l·∫≠p t·ª©c
//         if (typeof window.sherpaOnnx !== "undefined") {
//             start();
//         } else {
//             console.warn("‚è≥ ƒêang ƒë·ª£i th∆∞ vi·ªán Sherpa-ONNX t·∫£i xong...");
//             checkInterval = setInterval(checkLibrary, 500);
//         }
//     } else {
//         // N·∫øu enabled = false, d·ªçn d·∫πp ngay
//         internalCleanup();
//     }

//     return () => {
//       aborted = true; // ƒê·∫∑t c·ªù h·ªßy ƒë·ªÉ ch·∫∑n c√°c h√†m async ƒëang ch·∫°y d·ªü
//       if (checkInterval) clearInterval(checkInterval);
//       internalCleanup();
//     };
  
//   }, [enabled, roomId, socket, displayName]); 
// }

import { useEffect, useRef, useState } from "react";
import type { Socket } from "socket.io-client";

declare global {
  interface Window {
    sherpaOnnx: any;
  }
}

export function useZipformerVI(
  socket: Socket | null,
  roomId: string | null,
  displayName: string,
  enabled: boolean
) {
  const [isModelReady, setIsModelReady] = useState(false);
  
  const recognizerRef = useRef<any>(null);
  const audioCtxRef = useRef<AudioContext | null>(null);
  const workletRef = useRef<AudioWorkletNode | null>(null);
  const streamRef = useRef<any>(null);
  const isInitializingRef = useRef<boolean>(false);
  const lastTextRef = useRef<string>("");

  useEffect(() => {
    let aborted = false;
    let checkInterval: ReturnType<typeof setInterval>;

    const internalCleanup = () => {
      console.log("üßπ ƒêang d·ªçn d·∫πp t√†i nguy√™n Zipformer...");
      if (workletRef.current) {
        workletRef.current.port.onmessage = null;
        workletRef.current.disconnect();
        workletRef.current = null;
      }
      if (streamRef.current) {
        try { streamRef.current.free(); } catch (e) {}
        streamRef.current = null;
      }
      if (recognizerRef.current) {
        try { recognizerRef.current.free(); } catch (e) {}
        recognizerRef.current = null;
      }
      if (audioCtxRef.current) {
        if (audioCtxRef.current.state !== "closed") {
          audioCtxRef.current.close();
        }
        audioCtxRef.current = null;
      }
      isInitializingRef.current = false;
      if (!aborted) setIsModelReady(false);
    };

    const start = async () => {
      // LOG 1: Ki·ªÉm tra xem h√†m start c√≥ ƒë∆∞·ª£c g·ªçi kh√¥ng
      console.log("üöÄ [Zipformer] B·∫Øt ƒë·∫ßu kh·ªüi ƒë·ªông...");
      
      if (isInitializingRef.current || aborted) {
          console.warn("‚ö†Ô∏è [Zipformer] ƒêang kh·ªüi t·∫°o ho·∫∑c ƒë√£ b·ªã h·ªßy, b·ªè qua.");
          return;
      }
      
      try {
        isInitializingRef.current = true;
        const sherpa = window.sherpaOnnx;

        // --- B∆Ø·ªöC 1: T·∫†O AUDIO CONTEXT ---
        const audioCtx = new AudioContext({ sampleRate: 16000 });
        audioCtxRef.current = audioCtx;
        
        // QUAN TR·ªåNG: Ki·ªÉm tra v√† Resume n·∫øu b·ªã treo
        if (audioCtx.state === 'suspended') {
            console.warn("‚ö†Ô∏è [Zipformer] AudioContext ƒëang b·ªã treo (suspended). ƒêang th·ª≠ resume...");
            await audioCtx.resume();
        }
        console.log("üîä [Zipformer] AudioContext State:", audioCtx.state);

        // Load Worklet
        console.log("üìÇ [Zipformer] ƒêang t·∫£i audio-processor.js...");
        try {
            await audioCtx.audioWorklet.addModule("/audio-processor.js");
            console.log("‚úÖ [Zipformer] T·∫£i audio-processor.js th√†nh c√¥ng");
        } catch (e) {
            console.error("‚ùå [Zipformer] Kh√¥ng t√¨m th·∫•y file /audio-processor.js trong public!", e);
            throw e;
        }

        if (aborted) { internalCleanup(); return; }

        // --- B∆Ø·ªöC 2: K·∫æT N·ªêI MICRO ---
        console.log("üé§ [Zipformer] ƒêang xin quy·ªÅn Micro...");
        const mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        const track = mediaStream.getAudioTracks()[0];
        console.log(`üé§ [Zipformer] Micro OK: ${track.label} (Enabled: ${track.enabled})`);

        if (aborted) {
            mediaStream.getTracks().forEach(t => t.stop());
            internalCleanup();
            return;
        }

        const source = audioCtx.createMediaStreamSource(mediaStream);
        const worklet = new AudioWorkletNode(audioCtx, "audio-processor");
        workletRef.current = worklet;
        
        source.connect(worklet);
        worklet.connect(audioCtx.destination);
        console.log("üîó [Zipformer] ƒê√£ n·ªëi d√¢y: Micro -> Worklet -> Speaker");

        // --- B∆Ø·ªöC 3: LOAD MODEL ---
        console.log("üß† [Zipformer] ƒêang t·∫£i Model (c√≥ th·ªÉ m·∫•t v√†i gi√¢y)...");
        const recognizer = sherpa.createOnlineRecognizer({
          modelConfig: {
            zipformer: {
              encoder: "/models/zipformer-vi/encoder-epoch-12-avg-8.onnx",
              decoder: "/models/zipformer-vi/decoder-epoch-12-avg-8.onnx",
              joiner: "/models/zipformer-vi/joiner-epoch-12-avg-8.onnx",
            },
            tokens: "/models/zipformer-vi/tokens.txt",
            provider: "wasm",
            numThreads: 2,
            debug: true, // B·∫≠t ch·∫ø ƒë·ªô debug c·ªßa Sherpa
          },
          decodingMethod: "greedy_search",
          enableEndpoint: true,
        });

        recognizerRef.current = recognizer;
        streamRef.current = recognizer.createStream();
        console.log("üéâ [Zipformer] Model ƒë√£ s·∫µn s√†ng!");

        // --- B∆Ø·ªöC 4: X·ª¨ L√ù S·ª∞ KI·ªÜN ---
        worklet.port.onmessage = (e) => {
          if (aborted) return;
        

          const samples = e.data as Float32Array;

          if (recognizerRef.current && streamRef.current) {
            streamRef.current.acceptWaveform(16000, samples);
            
            while (recognizerRef.current.isReady(streamRef.current)) {
              recognizerRef.current.decode(streamRef.current);
            }

            const result = recognizerRef.current.getResult(streamRef.current);
            const text = result.text;

            if (text && text.length > 0) {
                // LOG KHI C√ì CH·ªÆ
                console.log("üéØ [AI NGHE]:", text);

                if (socket && socket.connected) {
                    const textToSend = text.trim();
                    if (textToSend !== lastTextRef.current) {
                        console.log(`üì§ [Socket] G·ª≠i: "${textToSend}" (User: ${displayName})`);
                        
                        socket.emit("send-subtitle", { 
                            roomId, 
                            text: textToSend,
                            displayName: displayName 
                        });
                        
                        lastTextRef.current = textToSend;

                        // Reset n·∫øu h·∫øt c√¢u
                        if (recognizerRef.current.isEndpoint(streamRef.current)) {
                            console.log("‚èπÔ∏è [AI] H·∫øt c√¢u (Endpoint detected).");
                            recognizerRef.current.reset(streamRef.current);
                            lastTextRef.current = "";
                        }
                    }
                } else {
                    console.warn("‚ö†Ô∏è [Socket] C√≥ ch·ªØ nh∆∞ng Socket ch∆∞a k·∫øt n·ªëi ho·∫∑c b·ªã null!");
                }
            }
          }
        };

        if (!aborted) {
            setIsModelReady(true);
        }
        isInitializingRef.current = false;

      } catch (err) {
        console.error("‚ùå [Zipformer] L·ªói CH·∫æT NG∆Ø·ªúI trong h√†m Start:", err);
        internalCleanup();
      }
    };

    // Ki·ªÉm tra th∆∞ vi·ªán v√† ch·∫°y
    if (enabled && socket && roomId) {
        const checkLibrary = () => {
            if (aborted) return;
           if (
              window.sherpaOnnx &&
              typeof window.sherpaOnnx.createOnlineRecognizer === "function"
            ) {
              start();
            } else {
                console.log("‚è≥ [Zipformer] ƒêang ƒë·ª£i window.sherpaOnnx...");
            }
        };

        if (typeof window.sherpaOnnx !== "undefined") {
            start();
        } else {
            checkInterval = setInterval(checkLibrary, 500);
        }
    } else {
        console.log("zzz [Zipformer] Ch∆∞a ƒë·ªß ƒëi·ªÅu ki·ªán ch·∫°y (enabled/socket/roomId thi·∫øu).");
        internalCleanup();
    }

    return () => {
      aborted = true;
      if (checkInterval) clearInterval(checkInterval);
      internalCleanup();
    };
  }, [enabled, roomId, socket, displayName]);

  return { isModelReady };
}